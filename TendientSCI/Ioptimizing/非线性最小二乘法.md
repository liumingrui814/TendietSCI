## 非线性最小二乘

对于同样的样本数据$(\boldsymbol{x}^{(i)},y^{(i)})_{i\in[1,N]}$, 其中$\boldsymbol{x} = \begin{bmatrix} x_1\\x_2\\...\\x_m\end{bmatrix}$，线性最小二乘是用线性方法拟合，即$\hat{y}=\boldsymbol{w}\boldsymbol{x}+b$, 而若拟合的函数变为一个非线性的函数$\hat{y}=h(x)$, 函数$h$的具体形式由一组新的参数$\boldsymbol{w}$表述，则新的残差平方和写为
$$
\begin{align}
f(\boldsymbol{w})=\frac{1}{2}\sum_{i=1}^Nr_j^2(\boldsymbol{w})
\end{align}
$$
其中$r(\boldsymbol{w})=\hat{y}-y=h(\boldsymbol{x})-y$表示每个项的残差函数，非线性最小二乘则本质上是解决最小化$f(\boldsymbol{w})$的函数最值问题（无约束最优化），因此用向量函数的一般方法求解，不需附会线性最小二乘方法的vectorization的模型。

对于无约束的解法，常见的是梯度法
$$
f(w+\Delta w)=f(w)+\nabla f(w)\Delta w+o(\Delta w)
$$
由于$\nabla f(w)\Delta w$类似内积，则在相同步长情况下，$\Delta w$的方向与$\nabla f(w)$相反时$\nabla f(w)\Delta w$下降最大，因而利用梯度获得下降方向进行$w=w+\Delta w$的方法称为梯度法。梯度法利用了一阶泰勒展开的信息，因此会存在比较大的抖动和不确定性。

### 高斯-牛顿方法

#### 高斯牛顿法的数学推导

牛顿法考虑了二阶的求导信息
$$
f(w+\Delta w)=f(w)+\nabla f(w)^T\Delta w+\frac{1}{2}\Delta w^T\nabla^2 f(w)\Delta w + o(\Delta w^2)
$$
讨论$\begin{align}
f(\boldsymbol{w})=\frac{1}{2}\sum_{i=1}^Nr_j^2(\boldsymbol{w})
\end{align}$作为损失函数情况下的牛顿方法。

首先计算梯度$\begin{align} \nabla f(\boldsymbol{w})=\begin{bmatrix} \frac{\partial f}{\partial \boldsymbol{w}_1}\\ \frac{\partial f}{\partial \boldsymbol{w}_2}\\.. \\  \frac{\partial f}{\partial \boldsymbol{w}_m}\end{bmatrix} \end{align}$, $\begin{align} \frac{\partial f}{\partial \boldsymbol{w}_1} = \sum_{j=1}^N\frac{\partial f}{\partial r_j} \frac{\partial r_j}{\partial \boldsymbol{w}_1}\end{align}$, 由$\frac{\partial f}{\partial r_j(\boldsymbol{w})}=r_j(\boldsymbol{w})$(因为是残差平方)，可得
$$
\begin{align} \nabla f(w)=\begin{bmatrix} \frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\.. \\  \frac{\partial f}{\partial x_m}\end{bmatrix}  = 
\begin{bmatrix} \sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_1}\\ \sum_{j=1}^N r_j(x)\frac{\partial r_j}{\partial x_2}\\.. \\  \sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_m}\end{bmatrix}

\end{align}
$$
对每行的元素如$\begin{align} \sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_1} \end{align}$, 可以写为$\sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_1} = [ \frac{\partial r_1}{\partial x_1}, \frac{\partial r_2}{\partial x_1},... ,  \frac{\partial r_n}{\partial x_1}]\begin{bmatrix} r_1(x)\\r_2(x)\\...\\r_n(x)\end{bmatrix}$,则这个式子按行延拓则有
$$
\begin{align} \nabla f(w)=\begin{bmatrix} \frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\.. \\  \frac{\partial f}{\partial x_m}\end{bmatrix}  = 
\begin{bmatrix} \sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_1}\\ \sum_{j=1}^N r_j(x)\frac{\partial r_j}{\partial x_2}\\.. \\  \sum_{j=1}^Nr_j(x)\frac{\partial r_j}{\partial x_m}\end{bmatrix} = 
\begin{bmatrix} 
\frac{\partial r_1}{\partial x_1}& \frac{\partial r_2}{\partial x_1}&... &  \frac{\partial r_n}{\partial x_1}\\
\frac{\partial r_1}{\partial x_2}& \frac{\partial r_2}{\partial x_2}&... &  \frac{\partial r_n}{\partial x_2}\\
... & ... & ... & ...\\
\frac{\partial r_1}{\partial x_m}& \frac{\partial r_2}{\partial x_m}&... &  \frac{\partial r_n}{\partial x_m}
\end{bmatrix}_{m\times n}
\begin{bmatrix} r_1(x)\\r_2(x)\\...\\r_n(x)\end{bmatrix}_{n\times 1}

\end{align}
$$
若令$\boldsymbol{r}(\boldsymbol{w})=[r_1(\boldsymbol{w}),r_2(\boldsymbol{w}),...,r_n(\boldsymbol{w})]^T$, 上述的$m\times n$矩阵就是Jacobi矩阵的转置$J^T(\boldsymbol{w})$,则有$\nabla f(\boldsymbol{w})=J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})$.

> 按照上述定义，$\boldsymbol{r}(\boldsymbol{w})$是个$m\to n$的多元函数，其一阶梯度总会对应到一个$n\times m$(自变量在列方向排列，应变量在行方向排列)的雅可比矩阵$J = [\frac{\partial f_i}{\partial x_j}]_{n\times m}$, 在遇到复合函数求导时总有$f'(w) = J^Tf'(r)$, 其中$J^T$是内层函数$r$的雅可比矩阵。如本问题，当多元函数$\boldsymbol{r}(\boldsymbol{w})$被定义出来后，残差平方也就成了一个vectorization化的表示方法，即$\begin{align}\|\boldsymbol{r}(\boldsymbol{w})\|_2 = \sum_{i=1}^Nr_j^2(\boldsymbol{w} )， f(\boldsymbol{r}) = \frac{1}{2}\|\boldsymbol{r}\|_2, \nabla f(\boldsymbol{r}) = \boldsymbol{r}(\boldsymbol{w})\end{align}$, 这很多时候也是简化连加表示方法的一种好方式，可以被借鉴

进而讨论二阶梯度$\nabla^2f(\boldsymbol{w})$, 二阶梯度即一阶梯度再对$\boldsymbol{w}$求导，
$$
\begin{align}
\nabla^2f(\boldsymbol{w}) &=(J^T(\boldsymbol{w}))'\boldsymbol{r}(\boldsymbol{w})+J^T(\boldsymbol{w})\boldsymbol{r}'(\boldsymbol{w})\\
&=
\begin{bmatrix} 
\frac{\partial r_1}{\partial x_1}& \frac{\partial r_2}{\partial x_1}&... &  \frac{\partial r_n}{\partial x_1}\\
\frac{\partial r_1}{\partial x_2}& \frac{\partial r_2}{\partial x_2}&... &  \frac{\partial r_n}{\partial x_2}\\
... & ... & ... & ...\\
\frac{\partial r_1}{\partial x_m}& \frac{\partial r_2}{\partial x_m}&... &  \frac{\partial r_n}{\partial x_m}
\end{bmatrix}_{m\times n}'
\boldsymbol{r}(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})\\
&=[\nabla r_1, \nabla r_2, ... , \nabla r_n]'\boldsymbol{r}(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})
= [\nabla r_1, \nabla r_2, ... , \nabla r_n]'
\begin{bmatrix}
r_1(\boldsymbol{w})\\r_2(\boldsymbol{w})\\...\\r_n(\boldsymbol{w})
\end{bmatrix}
+J^T(\boldsymbol{w})J(\boldsymbol{w})\\
&=\sum_{j=1}^NHessain(r_j)r_j(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})
\end{align}
$$

> 注意，对梯度求导本质上就是算梯度的雅可比矩阵（$\nabla f(w) = [\frac{\partial f}{\partial w_1},\frac{\partial f}{\partial w_2},...,\frac{\partial f}{\partial w_m} ]^T$也是多元函数，也有雅可比矩阵而且是$m\times m$方阵），也就是Hessian矩阵。同时注意区分$f(\boldsymbol{w})$的二阶梯度和$r$的Hessian矩阵是两个东西，不要搞混了

回到泰勒展开的方程：
$$
f(w+\Delta w)=f(w)+\nabla f(w)^T\Delta w+\frac{1}{2}\Delta w^T\nabla^2 f(w)\Delta w + o(\Delta w^2)\\
\nabla f(\boldsymbol{w})=J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w}) \\
\nabla^2f(\boldsymbol{w}) = \sum_{j=1}^NHessain(r_j)r_j(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})
\approx J^T(\boldsymbol{w})J(\boldsymbol{w})
$$
则使得$\frac{\partial f}{\partial \boldsymbol{\Delta w}}=0$, (展开函数是关于$\Delta \boldsymbol{w}$的函数，对其求导使之为0获得极小值)得到
$$
J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})\Delta\boldsymbol{w} = 0\\
\Delta\boldsymbol{w} = -(J^T(\boldsymbol{w})J(\boldsymbol{w}))^{-1}J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})
$$
这便是基于纯的最优化理论得到的非线性优化过程的参数更新模型，

#### 基于QR分解的高斯牛顿方法计算

$\Delta\boldsymbol{w} = -(J^T(\boldsymbol{w})J(\boldsymbol{w}))^{-1}J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})$的复杂度很高，因为$Jacobi$的求逆有较高的复杂度，而且这样的逆矩阵难以获得不错的数值稳定性，因此在实际计算时往往考虑用QR分解的方法进行高斯牛顿方法的简化与求解。

QR分解是一种能够将矩阵$A=A_{n\times m} (n\geq m)$分解为一个正交单元$Q$和一个上三角单元$R$的乘积的矩阵分解方法

- $full \space QR$(完全QR分解)： $A = Q_{n\times n}R_{n\times m}$, $R$是上三角矩阵，在$m$行之后三角矩阵到头，后面全是0，$Q$是正交方阵
- $reduced \space QR$（约化QR分解)：$A = Q_{n\times m}R_{m\times m}$，$R$是上三角方阵，$Q$每列向量正交（因为维度问题，$Q$不能被称为正交方阵），列正交满足$Q^TQ=E$

对于$m\to n$的函数$\boldsymbol{r}(\boldsymbol{w})$,其雅可比矩阵$n\times m$可以用QR分解，此时用到约化QR分解的方法对雅可比矩阵$J(\boldsymbol{m})$做QR分解
$$
\begin{align}
J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})+J^T(\boldsymbol{w})J(\boldsymbol{w})\Delta\boldsymbol{w} = 0\\
J^T(\boldsymbol{w})J(\boldsymbol{w})\Delta\boldsymbol{w} = -J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})\\
(QR)^T(QR)\Delta\boldsymbol{w} = -(QR)^T\boldsymbol{r}(\boldsymbol{w})\\
R^TQ^TQR\Delta\boldsymbol{w} = R^TR\Delta\boldsymbol{w} = -R^TQ^T\boldsymbol{r}(\boldsymbol{w})\\
\Delta\boldsymbol{w} = -R^{-1}Q^T\boldsymbol{r}(\boldsymbol{w})
\end{align}
$$
这样一来求逆矩阵的矩阵变为标准三角阵$R$，对于稀疏的矩阵求逆拥有更好的数值性能与更快的速度，是高斯牛顿法非常不错的解法。

因此在实际的代码实现中，对于样本$(\boldsymbol{x}^{(i)},y^{(i)})_{i\in[1,N]}$, 需要计算$\boldsymbol{r}(\boldsymbol{w}) = [r_1(\boldsymbol{x}^{(1)},y^{(1)}),r_2(\boldsymbol{x}^{(2)},y^{(2)}),...,r_n(\boldsymbol{x}^{(n)},y^{(n)})]|_{\boldsymbol{w}=w_0}$, 以及$\boldsymbol{r}(\boldsymbol{w})$的雅可比矩阵（可以理论推导）$J(\boldsymbol{w})$的QR分解，后续用$\Delta\boldsymbol{w} = -R^{-1}Q^T\boldsymbol{r}(\boldsymbol{w})$完成一次迭代的更新

### 列文伯格-马夸尔特（Levenberg-Marquardt）方法

#### 信赖域

从高斯牛顿法的推导中也可以看出，高斯牛顿法完全仰仗$f(\boldsymbol{w}+\Delta \boldsymbol{w})=f(\boldsymbol{w})+ \nabla f(\boldsymbol{w})^T\Delta \boldsymbol{w} + \frac{1}{2}\Delta \boldsymbol{w}^T\nabla^2f(\boldsymbol{w})\Delta \boldsymbol{w}$的结果，基于信赖域的算法提供了一种评估二阶泰勒展开对$f(\boldsymbol{w})$评估效果好坏的机制。

对于二阶泰勒展开的近似项$\boldsymbol{p}=f(\boldsymbol{w})+ \nabla f(\boldsymbol{w})^T\Delta \boldsymbol{w} + \frac{1}{2}\Delta \boldsymbol{w}^TB_\boldsymbol{w}\Delta \boldsymbol{w}$,  假设$B_\boldsymbol{w}$由较为简单的计算方法得到且能一定程度上近似$f$的二阶梯度$\nabla^2f(\boldsymbol{w})$, 信赖域方法仅考虑一个球形邻域内的下降情况，即：
$$
\begin{align}
\Omega_k = \{{\boldsymbol{w}_k+d}| \quad \|d\|\leq \Delta_k\}\\
\min_{d\in R^m} \quad \boldsymbol{p}(d), s.t. \|d\|\leq \Delta_k
\end{align}
$$
求解这个有约束的问题则可以得到一个下降方向$d$, 从而能够设计一个近似指标$\rho$
$$
\rho = \frac{f(\boldsymbol{w})-f(\boldsymbol{w}+d)}{\boldsymbol{p}(0)-\boldsymbol{p}(d)}
$$
即分子是真实函数的下降大小，分母是近似函数的下降大小，可知$\rho$越大说明当前的信赖域$\Delta_k$越保守，而$\rho$越小则说明当前的信赖域不可信。基于$\rho$的值能够使得迭代过程中$\Delta_k$可以被动态地调整。

#### Levenberg-Marquardt方法的求解

结合先前Gauss-Newton方法的近似$\nabla^2 f(\boldsymbol{w}) = \sum_j H(r_j)r_j(\boldsymbol{w})+J(\boldsymbol{w})^TJ(\boldsymbol{w}) \approx J(\boldsymbol{w})^TJ(\boldsymbol{w})$，这是个**半正定**的二阶梯度近似，容易出现奇异性（因为要求逆，因而应当能够保证这个矩阵是正定的，以得到更好的数值稳定性）

LM方法借用了二阶梯度用$J(\boldsymbol{w})^TJ(\boldsymbol{w})$近似的思路，但LM方法加了阻尼量$\frac{\lambda}{2}\Delta\boldsymbol{w}^T\boldsymbol{w}$用于控制二阶梯度项。即其利用
$$
\begin{align}
f(\boldsymbol{w}+\Delta \boldsymbol{w})=f(\boldsymbol{w})+ J(\boldsymbol{w})^Tr(\boldsymbol{w})\Delta \boldsymbol{w} + \frac{1}{2}\Delta \boldsymbol{w}^TJ(\boldsymbol{w})^TJ(\boldsymbol{w})\Delta \boldsymbol{w}+\frac{\lambda}{2}\Delta\boldsymbol{w}^T\boldsymbol{w}
\end{align}
$$
作为展开函数，同样使得增量项对$\Delta \boldsymbol{w}$求导为0得到
$$
\Delta\boldsymbol{w} = -(J^T(\boldsymbol{w})J(\boldsymbol{w})+\lambda I)^{-1}J^T(\boldsymbol{w})\boldsymbol{r}(\boldsymbol{w})
$$
从暴力数值的角度看，$\lambda$表示的是**对下降过程收敛方向**的控制：

- 当$\lambda$很大时，忽略$J^TJ$的影响，$J^Tr = \nabla f$, 这个式子的方向偏向负梯度
- 当$\lambda$很小时，上述式子就是高斯牛顿方法的迭代式子，方向偏向二阶梯度得到的方向
- 以$\rho = \frac{f(\boldsymbol{w})-f(\boldsymbol{w}+d)}{\boldsymbol{p}(0)-\boldsymbol{p}(d)}$为判据，当$\rho$大时，说明方向不错，但保守了，因此需要减小$\lambda$，以使得迭代更偏向高斯牛顿（高斯牛顿收敛快）
- 当$\rho$小时，说明方向很差，需要保守一点，不能用高斯牛顿，而是增大$\lambda$,使得算法更像是梯度下降

因此$\rho$和$\lambda$是负相关的，那么$\lambda$和信赖域算法又有什么关系呢？这就要回到最初的阻尼项：
$$
\begin{align}
f(\boldsymbol{w}+\Delta \boldsymbol{w})=f(\boldsymbol{w})+ J(\boldsymbol{w})^Tr(\boldsymbol{w})\Delta \boldsymbol{w} + \frac{1}{2}\Delta \boldsymbol{w}^TJ(\boldsymbol{w})^TJ(\boldsymbol{w})\Delta \boldsymbol{w}+\frac{\lambda}{2}\Delta\boldsymbol{w}^T\boldsymbol{w}
\end{align}
$$
不妨换种角度，将上述式子理解成拉格朗日函数，$\frac{\lambda}{2}(d^Td-\Delta)$的$\Delta$是个常量，不影响对$\Delta \boldsymbol{w}$的求导。那么$\lambda$则变成了与信赖域$\Delta$有关的量，$\|d(\lambda)\|$在大区间上是个减函数 (因为求导时有$(B+\lambda I)d +g= 0$)，且满足$\lambda(\|d(\lambda)\|-\Delta)=0$时才能够满足拉格朗日乘子法的极值条件，**因而当$\lambda$越大时，实际上对应的是信赖域越小时的带约束的最优化二阶项。**从而$\lambda$越大越保守。

基于数值分析$\lambda$和基于$lagrange$乘子分析$\lambda$是殊途同归的，只有从$lagrange$乘子的角度看这个问题才能够得到$LM$与信赖域的实际关系，而$\lambda \Delta \boldsymbol{w}^T\Delta \boldsymbol{w}$作为阻尼项加则是LM方法小改动，大改进的优点，这是在学习知识时都应该两面了解的。

### 基于QR分解的Gauss-Newton 实践

以拟合非线性函数：
$$
y = e^{ax^2+bx+c}
$$
为例，则容易知道拟合参数为$\boldsymbol{w} = [a,b,c]$, 残差函数为$r^{(i)}(\boldsymbol{w}) = e^{ax^{(i)2}+bx^{(i)}+c}-y^{(i)}$,其梯度为
$$
\frac{\partial r}{\partial \boldsymbol{w}} = 
\begin{bmatrix}
x^{(i)2}e^{ax^{(i)2}+bx^{(i)}+c} \\
x^{(i)}e^{ax^{(i)2}+bx^{(i)}+c}\\
e^{ax^{(i)2}+bx^{(i)}+c}
\end{bmatrix}
$$
则对于多元函数$\boldsymbol{r}(\boldsymbol{w}) = [r_1(\boldsymbol{w}),r_2(\boldsymbol{w}),...,r_N(\boldsymbol{w})]$,雅克比矩阵为$N\times 3$, 列方向为$\frac{\partial r}{\partial \boldsymbol{w}}^T$,行方向上为每个样本值对应的梯度。

则在得到雅克比矩阵$J$后继续计算$J$的约化形式QR分解，并用
$$
\Delta\boldsymbol{w} = -R^{-1}Q^T\boldsymbol{r}(\boldsymbol{w})
$$
更新参数，完成一次迭代。

注意到利用$QR$分解中约化$QR$分解不易用代码实现的问题（利用`Eigen`得到$N\times 3$的$QR$分解是$Q_{N\times N}R_{N \times 3}$, 此时$R$不能求逆，则可以回到上一步
$$
 R\Delta\boldsymbol{w} = -Q^T\boldsymbol{r}(\boldsymbol{w})
$$
将上述问题表述成$A\boldsymbol{x} = b$模式的解线性方程问题，进而进行$\Delta \boldsymbol{w}$的求解，因为$R$比较简单，这种方法也能够具备更好的数值稳定性和计算效力。

不过同时**注意到$full \space QR$与$reduced \space QR$本质上是一样的**。对于$N\times N$的$Q$, 得到$N\times m$的$R$, 由于$R$的后面几行都是0，即表示$Q$的第$m$列以后的所有正交基都没有参与到$R$有效三角部分的计算，只是做了满秩的延拓，因此$Q_{N\times m}$和$R_{m\times m}$才是$QR$分解的有效部分。

$reduced \space QR$**是$full \space QR$的简化切片**，是$full \space QR$的一部分（也就是说在用自己的方法算的时候不用把一整个$Q$算出来。

因此在实际计算时，同样可以调库计算full QR, 切片后即进行矩阵求逆，这比解方程要快。

具体代码可见`useEigen/GaussNewton.cpp`

