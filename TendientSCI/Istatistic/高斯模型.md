## 理解正态分布

> lmrlmrlmrlmr 2022/11/2

在学习《概率机器人》和一些统计学习的知识时，这些更高层的概率操作始终都是建立在正态分布的性质之上，这不禁让我们开始好奇正态分布函数：
$$
p(x)= \frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}
$$
为何会拥有诸多统计学和概率论上的优秀性质，以及这个函数的由来、其诸多性质的前因后果等，对这些理论的理解对于我们学习和掌握后续更多的概率学的知识具有重要意义。

#### 高斯函数

正太分布函数是在研究观测误差时被提出的，高斯发现在对一些物理量进行多次观测时其观测值的分布近似地满足某种分布情况，从而结合数学知识根据误差分布首当满足的先验性质得到了高斯函数。也即高斯函数是在研究**独立同分布的观测变量$x\sim \mathcal{D}(\mu,\sigma)$在连续空间中的分布情况**。

假设对物理量$x$做出若干观测$\{x_i\}$, 其观测值受真实世界的物理影响, 即$\{x_i\} = \{x_i |\theta \}$, 则考虑一个概率密度函数$f(e|\theta)$来描述$\{x_i\}$的观测误差$\{e_i\}$, 这个概率密度函数满足一些在真实世界合理的假设：

- $f(e|\theta)$是一个偶函数，在空间中可导（从而$f'(e|\theta)$是个奇函数）
- $f(e|\theta)$在$\|e\|$越大时越小，在$\|e\|=0$时最大
- $f(e|\theta)$恒正
- 在观测数量$\{x_i\}$足够多时，满足$\begin{align}\overline{x}=\frac{1}{N}\sum_{i=0}^N x_i\end{align} \to \mu$, 即大数定理。

> 注意：这几条性质都只能形容一个**观测**的规律，而并不适用于所有独立同分布的变量，如如下的一个随机变量：
>
> $p(x=1)=1/2, p(x=2)=1/3,p(x=3)=1/6$, 它并不满足上面的四条性质，多个$x$样本最终会使得样本的分布趋近于上式给出的分布，随机变量如何构造得到满足上面四点的样本$\{x_i\}$请参考中心极限定理

以上的几点性质都是可以直接由随机事件本身性质得到，并不算很强的描述。则在已知一个样本数量充足的观测样本$\{x_i\}$时，能够最好地形容$e_i$的分布函数**应该使得这个样本出现的似然率$p(\{x_i\}|\theta)$最大**，在$\{x_i\}$独立同分布的假设下，有似然函数：
$$
\begin{align}
L(\mu) = p(\{e_i\}|\theta) = p(\{e_i\}|\mu,\sigma)  = \prod_{i=1}^N p(x_i-\mu)
\end{align}
$$
且满足当$\begin{align}\overline{x}=\frac{1}{N}\sum_{i=0}^N x_i\end{align}$作为参数$\mu$的估计值时$L(\mu)$能取到最大值。

> 这里我们且先不讨论$p(x)$满足何种形制，而是默认$p(x)$满足某种形制做后续的讨论，假设$p(x)$就是$x$的概率密度函数，$\prod_{i=1}^N p(x_i-\mu)=L(\mu)$表示$x_i$的似然情况，$L$取到最大时从数学意义上就应该是$\mu=\bar{x}$的时候.
>
> 这里相当于我们已经直接给出了在$p(x)$存在的情况下$L(\mu)$取到最大的情形，并通过$L(\mu)$最大所具备的导数特性中$p(x)$表现出的性质得到$p(x)$满足的函数形制。

$L(\mu)$由于有连乘关系，故考虑一致单调的函数$\ln(L(\mu))$的极值。
$$
\frac{\partial \ln(L(u))}{\partial u} =\frac{\partial}{\partial \mu}\ln[\prod_{i=1}^N p(x_i-\mu)] = \frac{\partial}{\partial \mu}\sum_{i=0}^N \ln[p(x_i-\mu)] 
$$
考虑构造$\begin{align} g(x)=\frac{p'(x_i)}{p(x_i)} \end{align}$, 这是$\ln[p(x_i-\mu)]$的导函数，且根据对$p(x)$的预设，$g(x)$是个奇函数。则有
$$
\frac{\partial \ln(L(u))}{\partial u}|_{\mu=\bar{x}} = \frac{\partial}{\partial \mu}\sum_{i=0}^N \ln[p(x_i-\mu)]|_{\mu=\bar{x}} = -\sum_{i=0}^N g(x_i-\bar{x})=0
$$
由于$g(x)$是奇函数，$g(0)=0$, 上式满足$\sum_{i=0}^N (x_i-\bar{x})=0$, 对于任意$N，x_i$成立，即$g(x)$满足：
$$
g(x_1+x_2+...+x_k)=g(x_1)+g(x_2)+...+g(x_k)=0 
$$
做进一步泛化的描述，$g(x)$对于任意的$x,y,z$都满足$g(x+y)+g(z)=0, \space g(x)+g(y)+g(z)=0$, 也就是说对 任意的$x,y$对于$g(x)$都满足
$$
g(x)+g(y)=g(x+y)
$$
因为不论$x,y$如何取值，始终存在$z=-x-y$使得$z+x+y=0$满足$g(z)+g(x+y)=g(x)+g(y)+g(z)=g(x+y+z)=0$, 这个规律也被称为柯西函数方程。

满足$g(x)+g(y)=g(x+y)$的函数有且仅有一种，即线性函数，这也很好证明，假设$y=dx$, 该方程变为：
$$
\begin{align}
&g(x)+g(dx) = g(x+dx)\\
&g(x+dx)-g(x) = g(dx)\\
&\frac{g(x+dx)-g(x)}{dx} = \frac{g(dx)}{dx} = g'(0)
\end{align}
$$
即$g(x)$在任何地方的导数都等于$g'(0)$, 即$g(x)$是个线性函数（有关$g(x)$可不可导的严格证明在这里并不重要，因此不用用严谨的数学说明去解释这个问题）。$g(x)=cx+b$， 且由于$g(x)$是奇函数，$g(x)=cx$

> 在这里是高斯的证明方法假设性最强的地方，即在$\begin{align}\overline{x}=\frac{1}{N}\sum_{i=0}^N x_i\end{align} \to \mu$是合理假设的情况下（即样本$\{x_i\}$要能够很好地刻画分布$p$）$\begin{align}-\sum_{i=0}^N g(x_i-\bar{x})=0\end{align}$是个可以达到的方程，且对任意$\{x_i\}$都能做到，满足这个条件的只有最理想情况下的正态分布。在$\begin{align}\overline{x}=\frac{1}{N}\sum_{i=0}^N x_i\end{align} \to \mu$时$L(\mu)$取得最大值这个结论这对任意$p$,只要$\{x_i\}$能够正确刻画其分布就能成立。而此时$L(\mu)$求导能为0在一些函数（比如$g(x)=-x^3$)下就是一个难以成立的条件。因此我们说$g(x)+g(y)=g(x+y)$是这个证明方法（其实这里给出的就是高斯本人的证明方法）最具理想性和最具均匀性的假设结论，事实也证明正态分布确实是能够用这个假设去解示的。在[正态分布的前世今生 (上) | 统计之都 (cosx.org)](https://cosx.org/2013/01/story-of-normal-distribution-1)中给出了基于信息论、噪声原理、几何原理等不同角度对正态分布的推导和刻画。**而这些都能体现正态分布是基于“完全均匀”的前提出发推导得到的函数。**这是个很漂亮的函数。

从而可以得到下面这个常微分方程：
$$
g(x) = \frac{p'(x)}{p(x)} = cx
$$
解这个常微分方程能够得到$p(x)=b\exp(\frac{c}{2}x^2+k)$,且由于$p(x)$是个偶函数，可得$k=0$, 即得到$p(x)=b\exp(\frac{c}{2}x^2)$, 结合积分归一化(这时应该知道$c$是个负数）可以求得
$$
p(x) = \frac{\sqrt{-c}}{\sqrt{2\pi}}\exp(\frac{c}{2}x^2)
$$
这已经初步具备了正态分布的函数形制了，剩下一个$c$，$c$是$g(x)$的导数，根据方差的定义$\begin{align}Var(x)=\int_{-\infty}^{\infty}(x-\mu)^2f(x)dx \end{align}$可以求得上述高斯函数的方差是$1/\sqrt{-c}$, 也即$c = -\frac{1}{\sigma^2}$, 最终就能得到正态分布的高斯函数就是我们异常熟悉的形状：
$$
p(x)= \frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}
$$
总结一下可以发现，$p(x)$的期望和方差在函数里如何表现出来是由钟形函数的积分特性决定的（因为就是按定义推的），因此描述高斯分布深层是这个钟形函数的由来，高斯函数之所以满足负指数平方的钟形结构本质上又是因为分布函数的**导数特性**，而这种导数特性则是**独立同分布观测样本**的分布规律在**连续空间中基于均匀假设的一种表现**。不难发现我们在推导高斯函数时用到的四个根据 (对称，恒正，大数定律，从大到小) 本质上都是独立同分布的观测随机变量样本$\{x_i\}$的规律，在带入均匀性的假设后，在连续空间中即表现为高斯函数。

这么理解有助于我们对高斯函数进一步的认识。

#### 中心极限定理

期望为$\mu$, 方差为$\sigma^2$的独立同分布的随机变量的部分和$\sum_{i=1}^n x_i$在$n$充分大时近似地满足标准正态分布$\mathcal{N}(n\mu,n\sigma^2)$
$$
\frac{\sum_{i=1}^n x_i - n\mu}{\sigma\sqrt{n}} \sim \mathcal{N}(0,1)
$$

> 补充：对于独立的$x_1,x_2$有$Var(x_1+x_2)=Var(x_1)+Var(x_2)$
> $$
> \begin{align}
> Var(x_1+x_2)&=E((x_1+x_2-E(x_1+x_2))^2)\\
> &=E\{[(x_1-E(x_1))+(x_2-E(x_2))]^2\}\\
> &=E[(x_1-E(x))^2+(x_2-E(x))^2+2(x_1-E(x_1))(x_2-E(x_2))]\\
> &=Var(x_1)+Var(x_2)+2E[(x_1-E(x_1))(x_2-E(x_2))]
> \end{align}
> $$
> 由于对于独立的样本$x_1,x_2$满足$E(x_1)E(x_2)=E(x_2x_1)$, 最后一项拆开即$E(x_1x_2)-E(x_1)E(x_2)-E(x_1)E(x_2)+E(x_1)E(x_2)=0$， 从而有**两个独立随机变量和的方差等于方差的和。**中心极限定理里$\Sigma x$也正是满足这个规律，即方差为$n\sigma^2$, 期望为$n\mu$

注意这里是多次采样的均值（或者说和，本质上是一样的）满足逼近正态分布的性质。我们先前讨论了正态分布的高斯函数是怎么推出来的，高斯函数推导中$g(x)+g(y)=g(x+y)$描述的是高斯函数一致的均匀性（这当然是做了一些假设得到的结论），这种假设在物理世界中真实存在（身高、体重、物理量的观测这些），但并不易于用数学语言表述。

由于这里只是讨论高斯分布具有的优秀的性质，因此在分析中心极限定理时，其实只需要理解到为什么$\sum_{i=1}^N x_i$是满足均匀性的随机变量，直观理解确实如此，$\sum_{i=1}^N x_i$有期望$n\mu$和方差$n\sigma^2$,它波动的分布也是均匀的，因此其能够在这种均匀性有一定程度体现（样本数量充足）的情况下逼近正态分布，这是中心极限定理最重要需要了解的地方。